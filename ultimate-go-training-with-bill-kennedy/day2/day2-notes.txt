# Day 2: Multi-Threaded Programming

Don't write mutli-threaded software until you absolutely need to.
* the only time you need to add this to your software is when a single-threaded solution isn't fast enough.
* now you will have to balance the level of complexity for performance gain.

Semantics of multi-threading:
* what if we had to write our own OS scheduler?
    * multiple paths of execution at the same time
    * thread does the accounting and execution of the path
    * would be nice to give illusion that things are happening in parallel, even if it's not
    * threads will now have to have states: 
        1. runnable - asking for time on the hardware
        2. running - actually been placed on the hardware is executing
        3. waiting - waiting for something, don't care about this
    * Scheduler Period algo: any thread in a `runnable` state will get to run a share of scheduling period
        * if there is 1 thread in `runnable`, it gets full scheduling period (1000 ms)
        * if there are 2, then we split the scheduling time in half and the 2 share (each get 500ms)
        * if there are 10, then we continue to split (100ms each, but won't get as much work done)
        * if user keeps sending more threads, we still give the threads time to run but now we lose time per thread
        * it's also _not free_ to put/take thread from hardware - CONTEXT SWITCHING
            * in linux, it costs 1-2microsecs/12k instructions
            * when we get to 1000+ threads, we start losing time on the processing
    * Minimal Time Slice algo: no matter how many threads I have to schedule, I will not allow a thread to run less than Xms at a time
        * if the scheduler will cause this threshold to hit, extend the scheduling time
        * you will see work/requests being starved because they have to wait that whole scheduling time to be able to run
* 2 types of workloads:
    1. cpu bound
        * instructions set being executed will never naturally cause the thread to go to a waiting state
        * adding ints, fibonacci
        * each thread will use its full time slice every time
        * most efficient workloads to do and reason about concurrency
    2. io/blocking bound
        * can go to a waiting state
        * go and fetch the data from a url - network call is waiting state
* OS threads are preemptive schedulers with a flag of priority on every thread/event
    * the scheduler must make a hardware thread available for high priority events (like typing or mouse touch)
    * that's why some events, regardless of workload type, will be put in waiting
* you are also responsible for: syncronization and orchestration

Parallelism vs concurrency:
* use both of these to get better performance with the hardware
* parallelism: you can phsycially execute multiple paths at the same time
* concurrency: undefined, out-of-oreder execution (not random)
    * go specs do not define iteration order of a map - no definition of the order
* sometimes these 2 concepts need each other and sometimes they don't
    * ex: if we want to add up a list of ints, the single threaded algo will work better than one that sliced up the list into 4 parts and has 4 threads, because of the context SWITCHING
        * BUT if we add 4 more hardware threads, then the splitting algo will be faster because they can all work independently
        * cpu-bound workloads will run faster with concurrency if they have access to multiple threads
        * calculate: how many threads can you run in parallel?
        * but write the sequential algo first and then ask these questions
        * ex: can't sequentially solve bubble sort! because it needs a defined order
    * ex: io bound workload (containing urls), the 4-threaded algo will be faster because the urls will be idle for a time, and thus the context switching is working in our favor
        * how many threads to maximize performance? we don't know this!
        * answer-ish: magic number of how many threads to where only 1 is in a runnable state at any given time - impossible to find this
        * this is why thread-pool tech exists
    * 3 threads per hardware thread was always bill's magic number for solving 100k 
* when running multi-threaded software, you have to take advantage of the hardware that is there and scale accordingly
    * BUT there is always a breaking point, where even if we scale it, it will not be able to be able to handle the load
    * thread pool variables for IOCP: min threads per pool (1000), max threads per pool (2000), and concurrency value (0)
        * concurrency: as work gets added to the pool, don't add more threads to the pool than can be run in parallel
        * so now we have 2 active threads at any given time, but if using io bound workloads, then a lot of those loads are getting put in waiting and thus you could have thosands of threads but only 2 active at any given time
        * microsoft adjusts the concurrency number as the load runs, so that when it finds the number, it can run the pool as efficient as possible

Multi-threading in Go:
* go tries to eliminate the need for pooling as it comes to performance
    * pooling in Go should only be used for minimizing resources (funnelling resources)
* Go will only use MT when in a safe point (guarantee that no reading/writing to memory is taking place)
* When a Go program starts up, it will look for a map of how many threads the machine has to work with - then sets up the Goroutine on the Processor to Machine connection
    * GRQ - go's stack vs. LRQ the stack of the processor
    * minimum time slice for Goroutine is 10ms
* 4 classes of events to give scheduler opportunities to give context switches:
    1. keyword `go` - schedule the function to run concurrently, undefined
    2. garbage collection - huge chaos in scheduler
    3. system calls - cause Goroutine to move to waiting state
        * asynchronous: 
            * network calls (like a network call to `read`)
                * network polar - runs as a threadpool, starts as single threaded os pool, the only async calls are the network calls
                * it will attach the goroutine to the NP and let that handle the networking stuff
                * then a new Goroutine will start on the processor/machine, until the original goroutine is done and can come back and reattach
                * the state doesn't stay with you, it moves to the new place or is managed somewhere else
        * synchronous: 
            * from our programming model, the networking call is a synchronous call 
                * Go has turned async networking calls to sync calls, we get the benefits of async calls with the maintenance of state
                * Go handles that state management for you, which is mangnitudes simpler
            * for file reads, we Go uses sync calls, too, because we're reading from a file
                * we detach the goroutine and the machine and move that goroutine into the waiting state, then they will bring another machine/goroutine in and unblock the machine's queue
                * thread local storage - there is no guarantee that you will be on the same thread at the same time, so can't use this (unlike in c)
        * you can have up to 10,000 threads in 1-threaded go program before things start to go haywire
    4. Blocking calls - c libraries, syncronization, orchestration
        * system monitor runs with the scheduler:
            * ids that a goroutine hasn't implemented in under 20ns, and recoginzes that the goroutine has stalled (blocking the machine)
            * it will then kick off the same process as the syncronous call
* scheduler is a work stealing scheduler - when we are in parallel, we don't want machines to be in `waiting` and thus could try to steal work from other processors
    * there is also a limit for how many goroutines can be in the go run queue
* go has been able to turn io bound workloads at system level into cpu bound workloads at the operating system level because the Machines never go into a waiting state, even if the goroutines are switching contexts
    * then we can just through paths of execution at the problem, and everything will handle the efficiency for you
    * making pools irrelevant except for resource reduction

https://github.com/ardanlabs/gotraining/blob/master/topics/go/concurrency/goroutines/example1/example1.go
* you left main return, the program runs, so you need to manage your concurrency to make sure functions are done before the main function exists
* wait group will help with this - maintain a count of goroutines that are active - has an API of add/done/wait
    * can't make a copy of a wait group (always value semantics), must share state of that wait group
    * if you don't know how many goroutines you are going to create before you create them, we have a problem
    * if you want to stay out of trouble, keep the `add` and the `done` calls in the same sight, you are to do that
    * keep the `done` call out of the function that youre calling in the goroutine 
    * while the wait group is not 0, the main call will be blocked if you call the `wait` function - this is our GUARANTEE that all work from the child calls will be done
* there always has to be a guarantee for concurrency, or you are not handling it
* for tests, use `runtime.Gosched()` - this is NOT a guarantee, but it is good for tests
https://github.com/ardanlabs/gotraining/blob/master/topics/go/concurrency/goroutines/example2/example2.go
https://github.com/ardanlabs/gotraining/blob/master/topics/go/concurrency/goroutines/example3/example3.go

Data Races - condition in concurrency: 
* these always look random and are the worst bugs because they don't always show up
* https://youtu.be/WDIkqP4JbkE?t=1809 
    * what if you forget to use a mutex or an atomic instruction? results can be undefined, extremely difficult to know what the hardware is going to do so it doesn't know
    * if sync access to share data, then you need something to prevent individual threads to access to the same data at the same time - everything else will be taken care of
    * this is NOT free - has to execute instructions to make sure you see a coherent picture of memory  - you will take a performance hit
    * false sharing - what occurs when you have access patterns to memory that fall near one another on the same cache line but they don't conflict
        * this is BAD because scalability and performance will be affected 
        * the hardware uses value semantics - every core gets its own set of data to operate on
        * with small machines and small data sets, you won't see this; but you will notice with big data because you're not being sympathetic with the hardware
* https://github.com/ardanlabs/gotraining/blob/master/topics/go/concurrency/data_race/example1/example1.go
    * we find the code we need to change and we rip out all the logging except for one log
    * because of that log and because we weren't handling the concurrecny, we hit the print statement and it has caused a context switch
        * they aren't running atomically anymore, and we're giving the code a chance to context switch
        * go does have a race detector! 

Synchronization: 
* atomics, mutexes, read/write mutexes
    * atomics:
        * Atomic operations are more primitive than other synchronization techniques - lockless and generally implemented directly at hardware level
            * are often used in implementing other synchronization techniques
    * mutexs:
        * when you copy a mutex, it is a new mutex!
        * never put lock and unlock in different functions, should always be the same function
        * don't log in the mutex, get in and out as fast as possible because it blocks just about everything else!
        * if you see a function using a mutex, and there are multiple calls to lock, then that is a SMELL!
            * worse, the race tests won't even pick that up so it's really bad
        * if you want to protect the entire function, then you could have: `mu.Lock(); defer mu.Unlock()`
            * this will cause the entire function to take the thread
            * the latency cost is the execution of the entire function - where backpressure could happening
    * read/write mutex:
        * should only be used when you have a lot of reads, with occassional writes
            * ex: maps as caches
        * `mu.RLock()` - I promise I will just read, and the scheduler gives out a lot of these
        * if there is a write, the scheduler won't dole out any more read locks, wait for the reads to finish, do the write, and unlock everything to start again with the read locks
        * NOTE: if your code is not behaving using a read/write mutex, then you have probably switched the RLocks/Locks/RUnlocks/Unlocks
            * you will probably see some deadlocks or data corruption if this is happening





